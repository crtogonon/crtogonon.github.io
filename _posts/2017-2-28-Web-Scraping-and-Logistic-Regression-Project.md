As you can tell from the title, this project was an exercise in Web Scraping and Logistic Regression. The scenario here is that I am an employee at a contracting firm who is tasked with understanding what factors impact data science salaries, as well as find models that predict whether or not a job will pay a high salary. I did not start with a ready made data set in this situation. I had to get it myself.

# Getting the Data

Like I mentioned, there was no premade data set to work with for this project as I needed to get it myself. How? By using web scraping. I wrote code to extract job postings from indeed.com and I used the elements from those postings as my data. My algorithm pulled up to a thousand job postings from 20 different U.S. cities including New York, San Francisco, and Chicago. From those postings I obtained the job titles, company names, location, salary (if listed), and a short snip of the posting summary.

The web scrape started well and I initially had a dataset of over 17 thousand entries. However, I did not want to keep any duplicate postings so I dropped those and ended up with a little under 4 thousand unique job postings. I saved these as a csv file to use for the analysis part of the project. When I checked the data before cleaning, I found that of the approximately 4000 unique postings, only 240 had salary information posted. Since I can't build a model using incomplete information, I had to narrow my dataset to just those 240 postings. The risk, however, with such few samples is that the models I build may not be as strong as they could be with more data. Unfortunately companies rarely post salary numbers on postings, which makes it next to impossible to obtain more data. So I needed to build my models on the information that I had.

# Cleaning / Mining

Before I could start building any models with the data, there was a lot of cleaning that needed to be done. There was a lot of work done to get all of the columns in my dataframe into a workable format as well as create dummy variables for all of the categorical values. To begin with, all of the salaries listed were in a variety of different formats. Some were listed as one number while others were listed as ranges. I decided to change all of the ranges into the average of the 2 numbers in that range. Another problem was that while some salaries were listed as yearly, a number of postings had listings for monthly, weekly, and hourly pay. Under different circumstances, I would have dropped all data that was not listed as yearly, since they are not full time jobs (which is what we're looking for). But since I don't have a lot of salary information to work with, I decided to convert all non yearly pay rates to what they would be if they were full time yearly salaires. For the rest of the variables, I needed to put the strings into formats that I could work with before proceeding. I then created dummy variables for key words found in the job summary and titles as well as for the locations and a few of the most frequently occuring companies in the data set. Since I was ultimately trying to predict whether or not a job salary will be high and not the exact number of the salary, I created a dummy variable indicating a high salary. For my purposes, I defined a high salary as being greater than $100,000. In the end I had a dataset with 38 features to build my model on.

# Model Building

This was the far, far less frustrating part of the project. I wanted to see what models would be best for predicting a high salary from a job posting and I started with a Logistic Regression model. I used something called GridSearchCV in order to determine the optimal Logistic Regression model, and from that I ended up with a Logistic Regression which had an accuracy score of about 89 percent. I then wanted to see how well a KNeighborsClassifier would do. Again using GridSearch, I ended up with a model which also had an accuracy score of about 89 percent. This tells me that I could use either model to make predictions since they perform essentially equally.

# Insights

In order to find out which factors have the most impact on the salary of a job, I created a correlation matrix of all of the variables. From that matrix I found that the main factor is location. Jobs in cities like New York, San Francisco, and surprisingly, Charlotte showed a stronger correlation to high salaries. Also, the job level if listed on the posting showed a correlation as well, with jobs listed as senior level having correlation to higher salaries. If I were to make recommendations based on my findings, I would say that if someone is looking for high salary data science jobs, they should probably focus their search in the three cities that I mentioned as well as have the necessary experience to gain a position that is higher than entry level.

As I mentioned earlier, the fact that there are not a lot of job postings with salary information makes it more difficult to build a strong model for predicting salary range. Also, considering the large number of features I used for my models, there is a chance that the models could be overfit for the data. In this situation it would be more beneficial if we had more salary data that we could use to fit,test, and cross validate a model on.